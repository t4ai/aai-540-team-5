{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V - Feature Engineering and Feature Store\n",
    "\n",
    "University of San Diego - MS Applied AI\n",
    "\n",
    "AAI-540 Team 5\n",
    "\n",
    "October 21, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awswrangler in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.20.32 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (1.34.131)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.32 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (1.34.131)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (1.26.4)\n",
      "Requirement already satisfied: packaging<25.0,>=21.1 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (24.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (15.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /opt/conda/lib/python3.11/site-packages (from awswrangler) (4.12.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (1.26.19)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.23.32->awswrangler) (1.16.0)\n",
      "Requirement already satisfied: pyathena in /opt/conda/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /opt/conda/lib/python3.11/site-packages (from pyathena) (1.34.131)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /opt/conda/lib/python3.11/site-packages (from pyathena) (1.34.131)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from pyathena) (2023.6.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from pyathena) (2.9.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from pyathena) (8.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.26.4->pyathena) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.26.4->pyathena) (0.10.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.29.4->pyathena) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->pyathena) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Stored 's3_datalake_path_csv' (str)\n",
      "Stored 'local_data_path_csv' (str)\n",
      "Stored 's3_datalake_path_parquet' (str)\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "%run 0-Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \n",
      "    SELECT *\n",
      "    FROM\n",
      "        \"store_sales_feature_group_offline_1727227039\"\n",
      "    ORDER BY\n",
      "        store_nbr ASC, date ASC\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>cluster</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dow</th>\n",
       "      <th>sales_record_id</th>\n",
       "      <th>event_time</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01:1</td>\n",
       "      <td>1.727227e+09</td>\n",
       "      <td>2024-09-25 01:23:03.697</td>\n",
       "      <td>2024-09-25 01:18:05.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>7417.15</td>\n",
       "      <td>93.14</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-02:1</td>\n",
       "      <td>1.727227e+09</td>\n",
       "      <td>2024-09-25 01:23:07.224</td>\n",
       "      <td>2024-09-25 01:18:06.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>5873.24</td>\n",
       "      <td>92.97</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-03:1</td>\n",
       "      <td>1.727227e+09</td>\n",
       "      <td>2024-09-25 01:23:03.964</td>\n",
       "      <td>2024-09-25 01:18:07.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>5919.88</td>\n",
       "      <td>93.12</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-04:1</td>\n",
       "      <td>1.727227e+09</td>\n",
       "      <td>2024-09-25 01:23:03.859</td>\n",
       "      <td>2024-09-25 01:18:07.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>6318.79</td>\n",
       "      <td>93.12</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-05:1</td>\n",
       "      <td>1.727227e+09</td>\n",
       "      <td>2024-09-25 01:23:03.726</td>\n",
       "      <td>2024-09-25 01:18:08.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr    sales    oil  is_holiday  city  state  cluster  \\\n",
       "0  2013-01-01          1     0.00  93.14           1    18     12       13   \n",
       "1  2013-01-02          1  7417.15  93.14           0    18     12       13   \n",
       "2  2013-01-03          1  5873.24  92.97           0    18     12       13   \n",
       "3  2013-01-04          1  5919.88  93.12           0    18     12       13   \n",
       "4  2013-01-05          1  6318.79  93.12           1    18     12       13   \n",
       "\n",
       "   onpromotion  year  month  day  dow sales_record_id    event_time  \\\n",
       "0            0  2013      1    1    1    2013-01-01:1  1.727227e+09   \n",
       "1            0  2013      1    2    2    2013-01-02:1  1.727227e+09   \n",
       "2            0  2013      1    3    3    2013-01-03:1  1.727227e+09   \n",
       "3            0  2013      1    4    4    2013-01-04:1  1.727227e+09   \n",
       "4            0  2013      1    5    5    2013-01-05:1  1.727227e+09   \n",
       "\n",
       "                write_time      api_invocation_time  is_deleted  \n",
       "0  2024-09-25 01:23:03.697  2024-09-25 01:18:05.000       False  \n",
       "1  2024-09-25 01:23:07.224  2024-09-25 01:18:06.000       False  \n",
       "2  2024-09-25 01:23:03.964  2024-09-25 01:18:07.000       False  \n",
       "3  2024-09-25 01:23:03.859  2024-09-25 01:18:07.000       False  \n",
       "4  2024-09-25 01:23:03.726  2024-09-25 01:18:08.000       False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get features stacked by store then date\n",
    "sales_features_store = get_store_dataset_from_offline_feature_group(store_sales_feature_group)\n",
    "sales_features_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the unique stores\n",
    "unique_store_nbrs = sales_features_store['store_nbr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to build the target time series for each store\n",
    "def build_store_timeseries(store_sales, target_col):\n",
    "    unique_stores = store_sales['store_nbr'].unique()\n",
    "    store_timeseries = []\n",
    "    for store_nbr in unique_stores:\n",
    "        # get the sales data for this store and only keep the timestep and sales number\n",
    "        store_data = store_sales[store_sales['store_nbr'] == store_nbr]\n",
    "        store_data = store_data[['date', target_col]]\n",
    "\n",
    "        # convert to datetime and then to series with timestep = 1d\n",
    "        store_data['date'] = pd.to_datetime(store_data['date'])\n",
    "        store_data = store_data.set_index('date')\n",
    "        store_data = store_data.resample('D').sum()\n",
    "        store_ts = np.trim_zeros(store_data.iloc[:, 0], trim='f')\n",
    "\n",
    "        # add to list\n",
    "        store_timeseries.append(store_ts)    \n",
    "    return store_timeseries\n",
    "\n",
    "# helper function to write ts datasets to json\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))\n",
    "\n",
    "# helper function to write to S3\n",
    "s3 = boto3.resource(\"s3\")\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the target timeseries\n",
    "timeseries_stores_sales = build_store_timeseries(sales_features_store, 'sales')\n",
    "timeseries_stores_oil = build_store_timeseries(sales_features_store, 'oil')\n",
    "timeseries_stores_holidays = build_store_timeseries(sales_features_store, 'is_holiday')\n",
    "timeseries_stores_promotions = build_store_timeseries(sales_features_store, 'onpromotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set timeseries parameters\n",
    "# we use 2 hour frequency for the time series\n",
    "freq = \"1D\"\n",
    "\n",
    "# prediction window 7 days\n",
    "prediction_length = 7\n",
    "\n",
    "# window size/context length is 15 days\n",
    "context_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset date range: 2013-01-02 00:00:00 - 2017-08-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# inspect date range of the series\n",
    "print(\"Dataset date range: {} - {}\".format(timeseries_stores_sales[0].index.min(), timeseries_stores_sales[0].index.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days in series: 1686\n",
      "Days in training dataset: 1348\n",
      "Days in test data: 338\n",
      "Weeks in test data: 48\n"
     ]
    }
   ],
   "source": [
    "# calculate the total days in the date range so we can split at 80% mark\n",
    "import datetime\n",
    "series_start_date = timeseries_stores_sales[0].index.min()\n",
    "series_end_date = timeseries_stores_sales[0].index.max()\n",
    "delta = series_end_date - series_start_date\n",
    "\n",
    "# set training cutoff parameters\n",
    "training_series_day_count = int(delta.days * .8)\n",
    "start_training = series_start_date\n",
    "end_training = series_start_date + datetime.timedelta(days=training_series_day_count)\n",
    "\n",
    "# set test cutoff parameters\n",
    "start_test = end_training + datetime.timedelta(days=1)\n",
    "end_test = series_end_date\n",
    "test_days = delta.days - training_series_day_count\n",
    "test_weeks = int((delta.days - training_series_day_count) / 7)\n",
    "\n",
    "print(\"Total days in series: {}\".format(delta.days)) \n",
    "print(\"Days in training dataset: {}\".format(training_series_day_count))\n",
    "print(\"Days in test data: {}\".format(test_days))\n",
    "print(\"Weeks in test data: {}\".format(test_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_training),\n",
    "        \"target\": ts[start_training:end_training].tolist(),\n",
    "        \"dynamic_feat\": [\n",
    "            timeseries_stores_oil[i][start_training:end_training].tolist(),\n",
    "            timeseries_stores_holidays[i][start_training:end_training].tolist(),\n",
    "            timeseries_stores_promotions[i][start_training:end_training].tolist(),\n",
    "        ],\n",
    "    }\n",
    "    for i, ts in enumerate(timeseries_stores_sales)\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538\n"
     ]
    }
   ],
   "source": [
    "# Rolling weekly evaluations until end of test set\n",
    "num_test_windows = test_weeks - 1\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_training),\n",
    "        \"target\": ts[start_training : end_training + datetime.timedelta(days=k * prediction_length)].tolist(),\n",
    "        \"dynamic_feat\": [\n",
    "            timeseries_stores_oil[i][start_training : end_training + datetime.timedelta(days=k * prediction_length)].tolist(),\n",
    "            timeseries_stores_holidays[i][start_training : end_training + datetime.timedelta(days=k * prediction_length)].tolist(),\n",
    "            timeseries_stores_promotions[i][start_training : end_training + datetime.timedelta(days=k * prediction_length)].tolist(),\n",
    "        ],\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for i, ts in enumerate(timeseries_stores_sales)\n",
    "]\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write datasets to json files\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-053585949834'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_s3_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-053585949834/store-sales-forecasting/prepared/2024-09-29-05-27-53'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup S3 path to store train and test datasets\n",
    "s3_dataset_path = \"s3://{}/store-sales-forecasting/prepared/{}\".format(bucket, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "s3_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-1-053585949834/store-sales-forecasting/prepared/2024-09-29-05-27-53/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-1-053585949834/store-sales-forecasting/prepared/2024-09-29-05-27-53/test/test.json\n"
     ]
    }
   ],
   "source": [
    "# copy the train/test files to S3\n",
    "copy_to_s3(\"train.json\", s3_dataset_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_dataset_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
