{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part X.2 - Configure CI/CD Pipeline for DeepAR \n",
    "\n",
    "University of San Diego - MS Applied AI\n",
    "\n",
    "AAI-540 Team 5\n",
    "\n",
    "October 21, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Stored 's3_datalake_path_csv' (str)\n",
      "Stored 'local_data_path_csv' (str)\n",
      "Stored 's3_datalake_path_parquet' (str)\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "%run 0-Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init pipeline session\n",
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 buckets\n",
    "input_data_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/input\".format(bucket)\n",
    "batch_data_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/output\".format(bucket)\n",
    "train_job_output_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/model\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query f430db85-c9fa-48c5-9b03-8db93dc55f6f is being executed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \n",
      "    SELECT *\n",
      "    FROM\n",
      "        \"store_sales_feature_group_offline_1728336748\"\n",
      "    ORDER BY\n",
      "        store_nbr ASC, date ASC\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query f430db85-c9fa-48c5-9b03-8db93dc55f6f successfully executed.\n"
     ]
    }
   ],
   "source": [
    "# get latest feature store and write local\n",
    "input_local_file = './test-data/input_data.csv'\n",
    "sales_features_store = get_store_dataset_from_offline_feature_group(store_sales_feature_group)\n",
    "sales_features_store.to_csv(input_local_file)\n",
    "\n",
    "# set destination path in S3\n",
    "input_data_file = \"{}/input_data.csv\".format(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: test-data/input_data.csv to s3://sagemaker-us-east-1-343218227212/store-sales-forecasting/deepar/pipelines/train/input/input_data.csv\n"
     ]
    }
   ],
   "source": [
    "# copy validation dataset local\n",
    "!aws s3 cp $input_local_file $input_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model test:RMSE score: 3434.864990234375\n"
     ]
    }
   ],
   "source": [
    "# Load the best model information from our tuning job\n",
    "tuning_job_name = \"deepar-hyperparamete-241007-2220\"\n",
    "tuning_job_result = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "# get model details from best training job\n",
    "best_training_job_name = tuning_job_result[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "best_training_job = sm.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "\n",
    "# get the best RMSE score to use in pipeline\n",
    "best_rmse_metric = 0\n",
    "for metric_name in best_training_job['FinalMetricDataList']:\n",
    "    if(metric_name['MetricName'] == 'test:RMSE'):\n",
    "        best_rmse_metric = metric_name['Value']\n",
    "\n",
    "print(\"Best model test:RMSE score: {}\".format(best_rmse_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.2xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_path,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_path,\n",
    ")\n",
    "\n",
    "# this is the \n",
    "rmse_threshold = ParameterFloat(name=\"RmseThreshold\", default_value=best_rmse_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Docker Container for Custom Processing\n",
    "\n",
    "Fetch latest data from the offline feature store (Past 1 week)\n",
    "Transform in to JSONL Test Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘docker’: File exists\n",
      "Requirement already satisfied: sagemaker-studio-image-build in /opt/conda/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.10.44 in /opt/conda/lib/python3.11/site-packages (from sagemaker-studio-image-build) (1.35.37)\n",
      "Requirement already satisfied: sagemaker<3.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-studio-image-build) (2.232.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.37 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (1.35.37)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (0.10.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (4.21.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (2.2.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (0.1.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.26.19)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.36.0,>=1.35.37->boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (2.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0->sagemaker-studio-image-build) (3.20.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (1.10.17)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (13.7.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0->sagemaker-studio-image-build) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0->sagemaker-studio-image-build) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0->sagemaker-studio-image-build) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker<3.0->sagemaker-studio-image-build) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker<3.0->sagemaker-studio-image-build) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker<3.0->sagemaker-studio-image-build) (2024.7.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker<3.0->sagemaker-studio-image-build) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker<3.0->sagemaker-studio-image-build) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker<3.0->sagemaker-studio-image-build) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker<3.0->sagemaker-studio-image-build) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker<3.0->sagemaker-studio-image-build) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker<3.0->sagemaker-studio-image-build) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker<3.0->sagemaker-studio-image-build) (0.70.16)\n",
      "Requirement already satisfied: mlflow>=2.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (2.15.1)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.13.2)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.30.0)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.9.2)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.26.0)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (15.0.2)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.4.2)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (2.0.30)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.5.1)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<23 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (22.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (2.18.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.3.5)\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (2.33.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0->sagemaker-studio-image-build) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.47b0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0->sagemaker-studio-image-build) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p code\n",
    "!mkdir docker\n",
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install pandas==0.25.3 json boto3 sagemaker time awswrangler pyathena\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push docker container\n",
    "ecr_repository = 'deepar-processing-container'\n",
    "tag = ':latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Created ECR repository sagemaker-studio\n",
      "...................[Container] 2024/10/15 21:16:28.146560 Running on CodeBuild On-demand\n",
      "\n",
      "[Container] 2024/10/15 21:16:28.146574 Waiting for agent ping\n",
      "[Container] 2024/10/15 21:16:28.247523 Waiting for DOWNLOAD_SOURCE\n",
      "[Container] 2024/10/15 21:16:30.072083 Phase is DOWNLOAD_SOURCE\n",
      "[Container] 2024/10/15 21:16:30.112993 CODEBUILD_SRC_DIR=/codebuild/output/src1727034112/src\n",
      "[Container] 2024/10/15 21:16:30.113572 YAML location is /codebuild/output/src1727034112/src/buildspec.yml\n",
      "[Container] 2024/10/15 21:16:30.116997 Setting HTTP client timeout to higher timeout for S3 source\n",
      "[Container] 2024/10/15 21:16:30.117137 Processing environment variables\n",
      "[Container] 2024/10/15 21:16:30.161892 No runtime version selected in buildspec.\n",
      "[Container] 2024/10/15 21:16:30.175882 Moving to directory /codebuild/output/src1727034112/src\n",
      "[Container] 2024/10/15 21:16:30.177262 Unable to initialize cache download: no paths specified to be cached\n",
      "[Container] 2024/10/15 21:16:30.177763 Configuring ssm agent with target id: codebuild:57103657-cfd0-4328-afb8-b492dbe1dd82\n",
      "[Container] 2024/10/15 21:16:30.178028 Successfully updated ssm agent configuration\n",
      "[Container] 2024/10/15 21:16:30.178291 Registering with agent\n",
      "[Container] 2024/10/15 21:16:30.210925 Phases found in YAML: 3\n",
      "[Container] 2024/10/15 21:16:30.210940  BUILD: 4 commands\n",
      "[Container] 2024/10/15 21:16:30.210945  POST_BUILD: 3 commands\n",
      "[Container] 2024/10/15 21:16:30.210948  PRE_BUILD: 9 commands\n",
      "[Container] 2024/10/15 21:16:30.211261 Phase complete: DOWNLOAD_SOURCE State: SUCCEEDED\n",
      "[Container] 2024/10/15 21:16:30.211272 Phase context status code:  Message:\n",
      "[Container] 2024/10/15 21:16:30.277888 Entering phase INSTALL\n",
      "[Container] 2024/10/15 21:16:30.280478 Phase complete: INSTALL State: SUCCEEDED\n",
      "[Container] 2024/10/15 21:16:30.280492 Phase context status code:  Message:\n",
      "[Container] 2024/10/15 21:16:30.312877 Entering phase PRE_BUILD\n",
      "[Container] 2024/10/15 21:16:30.349488 Running command echo Logging in to Amazon ECR...\n",
      "Logging in to Amazon ECR...\n",
      "\n",
      "[Container] 2024/10/15 21:16:30.353707 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:33.362635 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 763104351884)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:33.930111 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 217643126080)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:34.764849 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 727897471807)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:35.663038 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 626614931356)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:36.493914 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 683313688378)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:37.372381 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 520713654638)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:38.166997 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 462105765813)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2024/10/15 21:16:38.997077 Phase complete: PRE_BUILD State: SUCCEEDED\n",
      "[Container] 2024/10/15 21:16:38.997099 Phase context status code:  Message:\n",
      "[Container] 2024/10/15 21:16:39.030526 Entering phase BUILD\n",
      "[Container] 2024/10/15 21:16:39.032039 Running command echo Build started on `date`\n",
      "Build started on Tue Oct 15 21:16:39 UTC 2024\n",
      "\n",
      "[Container] 2024/10/15 21:16:39.039841 Running command echo Building the Docker image...\n",
      "Building the Docker image...\n",
      "\n",
      "[Container] 2024/10/15 21:16:39.044737 Running command docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG -t deepar-processing-container docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/4 : FROM python:3.7-slim-buster\n",
      "3.7-slim-buster: Pulling from library/python\n",
      "8b91b88d5577: Pulling fs layer\n",
      "824416e23423: Pulling fs layer\n",
      "bbe2c2981082: Pulling fs layer\n",
      "7b6b68d15a5c: Pulling fs layer\n",
      "71f8f4db541d: Pulling fs layer\n",
      "7b6b68d15a5c: Waiting\n",
      "71f8f4db541d: Waiting\n",
      "824416e23423: Verifying Checksum\n",
      "824416e23423: Download complete\n",
      "bbe2c2981082: Verifying Checksum\n",
      "bbe2c2981082: Download complete\n",
      "7b6b68d15a5c: Verifying Checksum\n",
      "7b6b68d15a5c: Download complete\n",
      "8b91b88d5577: Verifying Checksum\n",
      "8b91b88d5577: Download complete\n",
      "71f8f4db541d: Verifying Checksum\n",
      "71f8f4db541d: Download complete\n",
      "8b91b88d5577: Pull complete\n",
      "824416e23423: Pull complete\n",
      "bbe2c2981082: Pull complete\n",
      "7b6b68d15a5c: Pull complete\n",
      "71f8f4db541d: Pull complete\n",
      "Digest: sha256:9bd2bfc822a533f99cbe6b1311d5bf0ff136f776ebac9b985407829f17278935\n",
      "Status: Downloaded newer image for python:3.7-slim-buster\n",
      " ---> 099f4583c701\n",
      "Step 2/4 : RUN pip3 install pandas==0.25.3 json boto3 sagemaker time awswrangler pyathena\n",
      " ---> Running in 682c0a885849\n",
      "Collecting pandas==0.25.3\n",
      "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 87.6 MB/s eta 0:00:00\n",
      "\u001b[91mERROR: Ignored the following versions that require a different python version: 1.4.0 Requires-Python >=3.8; 1.4.0rc0 Requires-Python >=3.8; 1.4.1 Requires-Python >=3.8; 1.4.2 Requires-Python >=3.8; 1.4.3 Requires-Python >=3.8; 1.4.4 Requires-Python >=3.8; 1.5.0 Requires-Python >=3.8; 1.5.0rc0 Requires-Python >=3.8; 1.5.1 Requires-Python >=3.8; 1.5.2 Requires-Python >=3.8; 1.5.3 Requires-Python >=3.8; 2.0.0 Requires-Python >=3.8; 2.0.0rc0 Requires-Python >=3.8; 2.0.0rc1 Requires-Python >=3.8; 2.0.1 Requires-Python >=3.8; 2.0.2 Requires-Python >=3.8; 2.0.3 Requires-Python >=3.8; 2.1.0 Requires-Python >=3.9; 2.1.0rc0 Requires-Python >=3.9; 2.1.1 Requires-Python >=3.9; 2.1.2 Requires-Python >=3.9; 2.1.3 Requires-Python >=3.9; 2.1.4 Requires-Python >=3.9; 2.2.0 Requires-Python >=3.9; 2.2.0rc0 Requires-Python >=3.9; 2.2.1 Requires-Python >=3.9; 2.2.2 Requires-Python >=3.9; 2.2.3 Requires-Python >=3.9\n",
      "\u001b[0m\u001b[91mERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "\u001b[0m\u001b[91mERROR: No matching distribution found for json\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mThe command '/bin/sh -c pip3 install pandas==0.25.3 json boto3 sagemaker time awswrangler pyathena' returned a non-zero code: 1\n",
      "\n",
      "[Container] 2024/10/15 21:16:46.517950 Command did not exit successfully docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG -t deepar-processing-container docker exit status 1\n",
      "[Container] 2024/10/15 21:16:46.523776 Phase complete: BUILD State: FAILED\n",
      "[Container] 2024/10/15 21:16:46.523832 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG -t deepar-processing-container docker. Reason: exit status 1\n",
      "[Container] 2024/10/15 21:16:46.556497 Entering phase POST_BUILD\n",
      "[Container] 2024/10/15 21:16:46.557504 Running command echo Build completed on `date`\n",
      "Build completed on Tue Oct 15 21:16:46 UTC 2024\n",
      "\n",
      "[Container] 2024/10/15 21:16:46.564735 Running command echo Pushing the Docker image...\n",
      "Pushing the Docker image...\n",
      "\n",
      "[Container] 2024/10/15 21:16:46.569753 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "The push refers to repository [343218227212.dkr.ecr.us-east-1.amazonaws.com/sagemaker-studio]\n",
      "An image does not exist locally with the tag: 343218227212.dkr.ecr.us-east-1.amazonaws.com/sagemaker-studio\n",
      "\n",
      "[Container] 2024/10/15 21:16:46.593754 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n",
      "[Container] 2024/10/15 21:16:46.596624 Phase complete: POST_BUILD State: FAILED\n",
      "[Container] 2024/10/15 21:16:46.596637 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n",
      "[Container] 2024/10/15 21:16:46.639627 Set report auto-discover timeout to 5 seconds\n",
      "[Container] 2024/10/15 21:16:46.639766 Expanding base directory path:  .\n",
      "[Container] 2024/10/15 21:16:46.641151 Assembling file list\n",
      "[Container] 2024/10/15 21:16:46.641163 Expanding .\n",
      "[Container] 2024/10/15 21:16:46.642632 Expanding file paths for base directory .\n",
      "[Container] 2024/10/15 21:16:46.642646 Assembling file list\n",
      "[Container] 2024/10/15 21:16:46.642650 Expanding **/*\n",
      "[Container] 2024/10/15 21:16:46.645332 No matching auto-discover report paths found\n",
      "[Container] 2024/10/15 21:16:46.645348 Report auto-discover file discovery took 0.005722 seconds\n",
      "[Container] 2024/10/15 21:16:46.645488 Phase complete: UPLOAD_ARTIFACTS State: SUCCEEDED\n",
      "[Container] 2024/10/15 21:16:46.645497 Phase context status code:  Message:\n",
      "\n",
      "Image URI: 343218227212.dkr.ecr.us-east-1.amazonaws.com/sagemaker-studio:latest\n"
     ]
    }
   ],
   "source": [
    "# build container\n",
    "!sm-docker build -t $ecr_repository docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: docker: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'deepar-processing-container' already exists in the registry with id '343218227212'\n"
     ]
    }
   ],
   "source": [
    "# Create ECR repository\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "!aws ecr create-repository --repository-name $ecr_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sm-docker [-h] {build} ...\n",
      "sm-docker: error: argument subcommand: invalid choice: 'tag' (choose from 'build')\n",
      "usage: sm-docker [-h] {build} ...\n",
      "sm-docker: error: argument subcommand: invalid choice: 'push' (choose from 'build')\n"
     ]
    }
   ],
   "source": [
    "# push docker image\n",
    "!sm-docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!sm-docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Data Pre-Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# helper function to build the target time series for each store\n",
    "def build_store_timeseries(store_sales, target_col):\n",
    "    unique_stores = store_sales['store_nbr'].unique()\n",
    "    store_timeseries = []\n",
    "    for store_nbr in unique_stores:\n",
    "        # get the sales data for this store and only keep the timestep and sales number\n",
    "        store_data = store_sales[store_sales['store_nbr'] == store_nbr]\n",
    "        store_data = store_data[['date', target_col]]\n",
    "\n",
    "        # convert to datetime and then to series with timestep = 1d\n",
    "        store_data['date'] = pd.to_datetime(store_data['date'])\n",
    "        \n",
    "        store_data = store_data.set_index('date')\n",
    "        store_data = store_data.resample('D').sum()\n",
    "        store_ts = store_data.iloc[:, 0]\n",
    "\n",
    "        # add to list\n",
    "        store_timeseries.append(store_ts)    \n",
    "    return store_timeseries\n",
    "\n",
    "# helper function to write ts datasets to json\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    \n",
    "    #load input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"input_data.csv\")\n",
    "    sales_features_store = pd.read_csv(input_data_path)\n",
    "\n",
    "    # calculate the total days in the date range so we can split at 80% mark\n",
    "    series_start_date = pd.to_datetime(sales_features_store['date'].min())\n",
    "    series_end_date = pd.to_datetime(sales_features_store['date'].max())\n",
    "    delta = series_end_date - series_start_date\n",
    "\n",
    "    # set training cutoff parameters\n",
    "    training_series_day_count = int(delta.days * .8)\n",
    "    start_training = series_start_date\n",
    "    end_training = series_start_date + datetime.timedelta(days=training_series_day_count)\n",
    "\n",
    "    # set test cutoff parameters\n",
    "    start_test = end_training + datetime.timedelta(days=1)\n",
    "    test_days = delta.days - training_series_day_count\n",
    "    test_weeks = int((delta.days - training_series_day_count) / 7)\n",
    "    val_weeks = int(test_weeks / 2)\n",
    "    test_weeks = val_weeks\n",
    "    end_test = start_test + datetime.timedelta(days=(test_weeks * 7))\n",
    "\n",
    "    # build the target timeseries\n",
    "    timeseries_stores_sales = build_store_timeseries(sales_features_store, 'sales')\n",
    "    timeseries_stores_oil = build_store_timeseries(sales_features_store, 'oil')\n",
    "    timeseries_stores_holidays = build_store_timeseries(sales_features_store, 'is_holiday')\n",
    "    timeseries_stores_promotions = build_store_timeseries(sales_features_store, 'onpromotion')\n",
    "\n",
    "    # capture the unique stores\n",
    "    unique_store_nbrs = sales_features_store['store_nbr'].unique()\n",
    "    deepar_prediction_length = 7\n",
    "\n",
    "    # generate training data\n",
    "    training_data = [\n",
    "        {\n",
    "            \"start\": str(start_training),\n",
    "            \"target\": ts[start_training:end_training].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][start_training:end_training].tolist(),\n",
    "                timeseries_stores_holidays[i][start_training:end_training].tolist(),\n",
    "                timeseries_stores_promotions[i][start_training:end_training].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "    print(len(training_data))\n",
    "\n",
    "    val_end = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "\n",
    "    # generate validation data\n",
    "    val_end = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "    val_data = [\n",
    "        {\n",
    "            \"start\": str(start_test),\n",
    "            \"target\": ts[start_test:val_end].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][start_test:val_end].tolist(),\n",
    "                timeseries_stores_holidays[i][start_test:val_end].tolist(),\n",
    "                timeseries_stores_promotions[i][start_test:val_end].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "    print(len(val_data))\n",
    "\n",
    "# Generate test data\n",
    "test_windows = test_weeks - 2\n",
    "gen_test_start = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "gen_test_end = gen_test_start + datetime.timedelta(days=(test_weeks*7))\n",
    "cw = 7\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(gen_test_start + datetime.timedelta(days=((k-1) * cw))),\n",
    "        \"target\": ts[(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) - 1)))].tolist(),\n",
    "        \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "        \"dynamic_feat\": [\n",
    "            timeseries_stores_oil[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "            timeseries_stores_holidays[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "            timeseries_stores_promotions[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "        ],\n",
    "    }\n",
    "    for k in range(1, test_windows + 1)\n",
    "    for i, ts in enumerate(timeseries_stores_sales)\n",
    "]\n",
    "\n",
    "print(len(test_data))\n",
    "\n",
    "# write datasets to json files\n",
    "write_dicts_to_file(\"/opt/ml/processing/train/train.json\", training_data)\n",
    "write_dicts_to_file(\"/opt/ml/processing/val/val.json\", val_data)\n",
    "write_dicts_to_file(\"/opt/ml/processing/test/test.json\", test_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup script processor\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri='343218227212.dkr.ecr.us-east-1.amazonaws.com/deepar-processing-container:latest',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    command=['python3'],\n",
    "    base_job_name=\"deepar-feature-process\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor_args = script_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data_file, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"val\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"DeepARFeatureProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DeepAR model with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# configure model image and output path\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "\n",
    "data_channels = {\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"json\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"val\"].S3Output.S3Uri,\n",
    "            content_type=\"json\",\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1d frequency for the time series\n",
    "deepar_freq = \"1D\"\n",
    "\n",
    "# prediction window 7 days\n",
    "deepar_prediction_length = 7\n",
    "\n",
    "# window size/context length is 15 days\n",
    "deepar_context_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# initialize estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.2xlarge\",\n",
    "    base_job_name=\"deepar-pipeline-train\",\n",
    "    output_path=train_job_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"time_freq\": deepar_freq,\n",
    "    \"epochs\": \"101\",\n",
    "    \"num_cells\": \"97\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"1024\",\n",
    "    \"learning_rate\": \"0.0006294407061415784\",\n",
    "    \"context_length\": \"3\",\n",
    "    \"prediction_length\": str(deepar_prediction_length),\n",
    "}\n",
    "\n",
    "# set hyperparameters to model\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model to train\n",
    "train_args = estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train step\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "step_train = TrainingStep(\n",
    "    name=\"DeepARPipelineTrain\",\n",
    "    step_args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Temporary Model to Use to Generate Predictions for Eval Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "from sagemaker.model import Model\n",
    "model = Model(\n",
    "    image_uri=image_name,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_eval_model = ModelStep(\n",
    "    name=\"DeepARPipelineCreateEvalModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Processing Step to Generate Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer\n",
    "from sagemaker.transformer import Transformer\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_eval_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=\"{}/eval_transorm\".format(batch_data_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TransformStep\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "transform_input = TransformInput(\n",
    "    data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri, \n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "step_transform_eval = TransformStep(\n",
    "    name=\"DeepARPipelineBatchEval\", transformer=transformer, inputs=transform_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Create Processing Step to Evaluate Model Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# helper function to load json\n",
    "def load_json_by_line(file_path):\n",
    "    results_raw = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            results_raw.append(json.loads(line))\n",
    "    return results_raw\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load input data\n",
    "    transform_path = f\"/opt/ml/processing/eval_transform/test.json.out\"\n",
    "    transform_input = load_json_by_line(transform_path)\n",
    "\n",
    "    # compile predictions with inputs\n",
    "    input_start_date = pd.to_datetime(transform_input[0]['start'])\n",
    "    predictions_available = False\n",
    "    predictions_data = []\n",
    "    for i, input in enumerate(transform_input):\n",
    "        start_date = pd.to_datetime(input['start'])\n",
    "        if(start_date > input_start_date):\n",
    "            predictions_available = True\n",
    "        store_nbr = int(input['cat'][0]) + 1\n",
    "        targets = input['target']\n",
    "\n",
    "        if(predictions_available):\n",
    "            predictions = transform_results[i]['mean']\n",
    "        else:\n",
    "            predictions = np.negative(np.ones(len(targets)))\n",
    "        \n",
    "        for j in range(7):\n",
    "            target = targets[j]\n",
    "            target_date = start_date + pd.Timedelta(days=j)\n",
    "            predictions_data.append([target_date, store_nbr, target, predictions[j]])\n",
    "\n",
    "    predictions_df = pd.DataFrame(columns=['date', 'store_nbr', 'true_sales', 'predicted_sales'], data=predictions_data)\n",
    "    predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
    "\n",
    "    # calculate RMSE\n",
    "    rmse = root_mean_squared_error(predictions_df[54:]['true_sales'], predictions_df[54:]['predicted_sales'])\n",
    "    std = np.std(predictions_df[54:]['true_sales'] - predictions_df[54:]['predicted_sales'])\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "sklearn_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "model_eval = ScriptProcessor(\n",
    "    image_uri=sklearn_image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"deepar-pipeline-model-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "model_eval_args = model_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_transform_eval.properties.TransformOutput.S3OutputPath,\n",
    "            destination=\"/opt/ml/processing/eval_transorm\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processing step with property file output\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_model_eval = ProcessingStep(\n",
    "    name=\"DeepARPipelineModelEval\",\n",
    "    step_args=model_eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"DeepARTrainDeployPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        rmse_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_eval_model, step_transform_eval, step_model_eval],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '84896cf5-b51c-4ae3-b82b-73356f007201',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '84896cf5-b51c-4ae3-b82b-73356f007201',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '93',\n",
       "   'date': 'Wed, 16 Oct 2024 16:18:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline/execution/iy2y6v8cgawr',\n",
       " 'PipelineExecutionDisplayName': 'execution-1729095514048',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'deepartraindeploypipeline',\n",
       "  'TrialName': 'iy2y6v8cgawr'},\n",
       " 'CreationTime': datetime.datetime(2024, 10, 16, 16, 18, 33, 984000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 10, 16, 16, 18, 33, 984000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:343218227212:user-profile/d-nzj1ohif3tlp/default-20241007T091581',\n",
       "  'UserProfileName': 'default-20241007T091581',\n",
       "  'DomainId': 'd-nzj1ohif3tlp',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::343218227212:assumed-role/AmazonSageMaker-ExecutionRole-20241007T091581/SageMaker',\n",
       "   'PrincipalId': 'AROAU72LGRAGGJLCV6WF3:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:343218227212:user-profile/d-nzj1ohif3tlp/default-20241007T091581',\n",
       "  'UserProfileName': 'default-20241007T091581',\n",
       "  'DomainId': 'd-nzj1ohif3tlp',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::343218227212:assumed-role/AmazonSageMaker-ExecutionRole-20241007T091581/SageMaker',\n",
       "   'PrincipalId': 'AROAU72LGRAGGJLCV6WF3:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': '7db028f3-ae97-417a-962c-335813c21ab7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7db028f3-ae97-417a-962c-335813c21ab7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1216',\n",
       "   'date': 'Wed, 16 Oct 2024 16:18:34 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'DeepARFeatureProcess',\n",
       "  'StartTime': datetime.datetime(2024, 10, 16, 16, 18, 35, 308000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Executing',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:343218227212:processing-job/pipelines-iy2y6v8cgawr-DeepARFeatureProcess-UVWP18i9HP'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
