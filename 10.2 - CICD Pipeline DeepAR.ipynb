{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part X.2 - Configure CI/CD Pipeline for DeepAR \n",
    "\n",
    "University of San Diego - MS Applied AI\n",
    "\n",
    "AAI-540 Team 5\n",
    "\n",
    "October 21, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Stored 's3_datalake_path_csv' (str)\n",
      "Stored 'local_data_path_csv' (str)\n",
      "Stored 's3_datalake_path_parquet' (str)\n"
     ]
    }
   ],
   "source": [
    "# setup environment\n",
    "%run 0-Environment_Setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init pipeline session\n",
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepar-hyperparamete-241007-2220-007-b18b6b1e'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup S3 buckets\n",
    "input_data_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/input\".format(bucket)\n",
    "batch_data_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/output\".format(bucket)\n",
    "train_job_output_path = \"s3://{}/store-sales-forecasting/deepar/pipelines/train/model\".format(bucket)\n",
    "\n",
    "# Get the current best registered model name\n",
    "registered_model_group_name = \"deepar-store-sales-prediction\"\n",
    "model_packages = sm.list_model_packages(\n",
    "    ModelPackageGroupName='deepar-store-sales-prediction', SortBy=\"CreationTime\", SortOrder=\"Descending\")\n",
    "model_package = model_packages[\"ModelPackageSummaryList\"][0]\n",
    "model_package = sm.describe_model_package(ModelPackageName=model_package[\"ModelPackageArn\"])\n",
    "model_data = model_package[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n",
    "registered_model_name = model_data.rsplit(\"/\", 3)[1]\n",
    "registered_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \n",
      "    SELECT *\n",
      "    FROM\n",
      "        \"store_sales_feature_group_offline_1728336748\"\n",
      "    ORDER BY\n",
      "        store_nbr ASC, date ASC\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query dedc8ae0-aedc-454e-80ea-110f449a9113 is being executed.\n",
      "INFO:sagemaker:Query dedc8ae0-aedc-454e-80ea-110f449a9113 successfully executed.\n"
     ]
    }
   ],
   "source": [
    "# get latest feature store and write local\n",
    "input_local_file = './test-data/input_data.csv'\n",
    "sales_features_store = get_store_dataset_from_offline_feature_group(store_sales_feature_group)\n",
    "sales_features_store.to_csv(input_local_file)\n",
    "\n",
    "# set destination path in S3\n",
    "input_data_file = \"{}/input_data.csv\".format(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: test-data/input_data.csv to s3://sagemaker-us-east-1-343218227212/store-sales-forecasting/deepar/pipelines/train/input/input_data.csv\n"
     ]
    }
   ],
   "source": [
    "# copy validation dataset local\n",
    "!aws s3 cp $input_local_file $input_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model test:RMSE score: 3434.864990234375\n"
     ]
    }
   ],
   "source": [
    "# Load the best model information from our tuning job\n",
    "tuning_job_name = \"deepar-hyperparamete-241007-2220\"\n",
    "tuning_job_result = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "# get model details from best training job\n",
    "best_training_job_name = tuning_job_result[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "best_training_job = sm.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "\n",
    "# get the best RMSE score to use in pipeline\n",
    "best_rmse_metric = 0\n",
    "for metric_name in best_training_job['FinalMetricDataList']:\n",
    "    if(metric_name['MetricName'] == 'test:RMSE'):\n",
    "        best_rmse_metric = metric_name['Value']\n",
    "\n",
    "print(\"Best model test:RMSE score: {}\".format(best_rmse_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.2xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_path,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_path,\n",
    ")\n",
    "\n",
    "# this is the metric we will measure model performance\n",
    "rmse_threshold = ParameterFloat(name=\"RmseThreshold\", default_value=best_rmse_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Docker Container for Custom Processing\n",
    "\n",
    "Fetch latest data from the offline feature store (Past 1 week)\n",
    "Transform in to JSONL Test Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install --upgrade pip\n",
    "RUN pip3 install -U pandas boto3 sagemaker awswrangler pyathena\n",
    "RUN pip3 install -U scikit-learn==1.4\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Data Pre-Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# helper function to build the target time series for each store\n",
    "def build_store_timeseries(store_sales, target_col):\n",
    "    unique_stores = store_sales['store_nbr'].unique()\n",
    "    store_timeseries = []\n",
    "    for store_nbr in unique_stores:\n",
    "        # get the sales data for this store and only keep the timestep and sales number\n",
    "        store_data = store_sales[store_sales['store_nbr'] == store_nbr]\n",
    "        store_data = store_data[['date', target_col]]\n",
    "\n",
    "        # convert to datetime and then to series with timestep = 1d\n",
    "        store_data['date'] = pd.to_datetime(store_data['date'])\n",
    "        \n",
    "        store_data = store_data.set_index('date')\n",
    "        store_data = store_data.resample('D').sum()\n",
    "        store_ts = store_data.iloc[:, 0]\n",
    "\n",
    "        # add to list\n",
    "        store_timeseries.append(store_ts)    \n",
    "    return store_timeseries\n",
    "\n",
    "# helper function to write ts datasets to json\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    \n",
    "    #load input data\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", \"input_data.csv\")\n",
    "    sales_features_store = pd.read_csv(input_data_path)\n",
    "\n",
    "    # calculate the total days in the date range so we can split at 80% mark\n",
    "    series_start_date = pd.to_datetime(sales_features_store['date'].min())\n",
    "    series_end_date = pd.to_datetime(sales_features_store['date'].max())\n",
    "    delta = series_end_date - series_start_date\n",
    "\n",
    "    # set training cutoff parameters\n",
    "    training_series_day_count = int(delta.days * .8)\n",
    "    start_training = series_start_date\n",
    "    end_training = series_start_date + datetime.timedelta(days=training_series_day_count)\n",
    "\n",
    "    # set test cutoff parameters\n",
    "    start_test = end_training + datetime.timedelta(days=1)\n",
    "    test_days = delta.days - training_series_day_count\n",
    "    test_weeks = int((delta.days - training_series_day_count) / 7)\n",
    "    val_weeks = int(test_weeks / 2)\n",
    "    test_weeks = val_weeks\n",
    "    end_test = start_test + datetime.timedelta(days=(test_weeks * 7))\n",
    "\n",
    "    # build the target timeseries\n",
    "    timeseries_stores_sales = build_store_timeseries(sales_features_store, 'sales')\n",
    "    timeseries_stores_oil = build_store_timeseries(sales_features_store, 'oil')\n",
    "    timeseries_stores_holidays = build_store_timeseries(sales_features_store, 'is_holiday')\n",
    "    timeseries_stores_promotions = build_store_timeseries(sales_features_store, 'onpromotion')\n",
    "\n",
    "    # capture the unique stores\n",
    "    unique_store_nbrs = sales_features_store['store_nbr'].unique()\n",
    "    deepar_prediction_length = 7\n",
    "\n",
    "    # generate training data\n",
    "    training_data = [\n",
    "        {\n",
    "            \"start\": str(start_training),\n",
    "            \"target\": ts[start_training:end_training].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][start_training:end_training].tolist(),\n",
    "                timeseries_stores_holidays[i][start_training:end_training].tolist(),\n",
    "                timeseries_stores_promotions[i][start_training:end_training].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "    print(len(training_data))\n",
    "\n",
    "    val_end = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "\n",
    "    # generate validation data\n",
    "    val_end = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "    val_data = [\n",
    "        {\n",
    "            \"start\": str(start_test),\n",
    "            \"target\": ts[start_test:val_end].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][start_test:val_end].tolist(),\n",
    "                timeseries_stores_holidays[i][start_test:val_end].tolist(),\n",
    "                timeseries_stores_promotions[i][start_test:val_end].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "    print(len(val_data))\n",
    "\n",
    "    # Generate test data\n",
    "    test_windows = test_weeks - 2\n",
    "    gen_test_start = start_test + datetime.timedelta(days=(val_weeks*7))\n",
    "    gen_test_end = gen_test_start + datetime.timedelta(days=(test_weeks*7))\n",
    "    cw = 7\n",
    "\n",
    "    test_data = [\n",
    "        {\n",
    "            \"start\": str(gen_test_start + datetime.timedelta(days=((k-1) * cw))),\n",
    "            \"target\": ts[(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) - 1)))].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "                timeseries_stores_holidays[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "                timeseries_stores_promotions[i][(gen_test_start + datetime.timedelta(days=((k-1) * cw))) : (gen_test_start + datetime.timedelta(days=((k * cw) + deepar_prediction_length - 1)))].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for k in range(1, test_windows + 1)\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "\n",
    "    print(len(test_data))\n",
    "\n",
    "    # generate forecast data\n",
    "    forecast_end_date = pd.to_datetime(sales_features_store['date'].max())\n",
    "    forecast_start_date = forecast_end_date - datetime.timedelta(days=6)\n",
    "\n",
    "    forecast_data = [\n",
    "        {\n",
    "            \"start\": str(forecast_start_date),\n",
    "            \"target\": ts[forecast_start_date:forecast_end_date].tolist(),\n",
    "            \"cat\": [int(unique_store_nbrs[i]) - 1],\n",
    "            \"dynamic_feat\": [\n",
    "                timeseries_stores_oil[i][forecast_start_date:forecast_end_date].tolist(),\n",
    "                timeseries_stores_holidays[i][forecast_start_date:forecast_end_date].tolist(),\n",
    "                timeseries_stores_promotions[i][forecast_start_date:forecast_end_date].tolist(),\n",
    "            ],\n",
    "        }\n",
    "        for i, ts in enumerate(timeseries_stores_sales)\n",
    "    ]\n",
    "\n",
    "    print(len(forecast_data))\n",
    "\n",
    "    # write datasets to json files\n",
    "    write_dicts_to_file(\"/opt/ml/processing/train/train.json\", training_data)\n",
    "    write_dicts_to_file(\"/opt/ml/processing/val/val.json\", val_data)\n",
    "    write_dicts_to_file(\"/opt/ml/processing/test/test.json\", test_data)\n",
    "    write_dicts_to_file(\"/opt/ml/processing/forecast/forecast.json\", forecast_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup script processor\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri='343218227212.dkr.ecr.us-east-1.amazonaws.com/deepar-processing-container:latest',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    command=['python3'],\n",
    "    base_job_name=\"deepar-feature-process\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor_args = script_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data_file, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"val\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"forecast\", source=\"/opt/ml/processing/forecast\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"DeepARFeatureProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train DeepAR model with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# configure model image and output path\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "\n",
    "data_channels = {\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"json\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"val\"].S3Output.S3Uri,\n",
    "            content_type=\"json\",\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1d frequency for the time series\n",
    "deepar_freq = \"1D\"\n",
    "\n",
    "# prediction window 7 days\n",
    "deepar_prediction_length = 7\n",
    "\n",
    "# window size/context length is 15 days\n",
    "deepar_context_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# initialize estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.2xlarge\",\n",
    "    base_job_name=\"deepar-pipeline-train\",\n",
    "    output_path=train_job_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"time_freq\": deepar_freq,\n",
    "    \"epochs\": \"101\",\n",
    "    \"num_cells\": \"97\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"1024\",\n",
    "    \"learning_rate\": \"0.0006294407061415784\",\n",
    "    \"context_length\": \"3\",\n",
    "    \"prediction_length\": str(deepar_prediction_length),\n",
    "}\n",
    "\n",
    "# set hyperparameters to model\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model to train\n",
    "train_args = estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train step\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "step_train = TrainingStep(\n",
    "    name=\"DeepARPipelineTrain\",\n",
    "    step_args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Temporary Model to Use to Generate Predictions for Eval Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "from sagemaker.model import Model\n",
    "model = Model(\n",
    "    image_uri=image_name,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_eval_model = ModelStep(\n",
    "    name=\"DeepARPipelineCreateEvalModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Batch Processing Step to Generate Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer\n",
    "from sagemaker.transformer import Transformer\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_eval_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=\"{}/eval_transorm\".format(batch_data_path),\n",
    "    assemble_with=\"Line\",\n",
    "    env={\n",
    "        \"DEEPAR_INFERENCE_CONFIG\": json.dumps({\n",
    "            \"output_types\": [\"mean\"],\n",
    "        })\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TransformStep\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "transform_input = TransformInput(\n",
    "    data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri, \n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "step_transform_eval = TransformStep(\n",
    "    name=\"DeepARPipelineBatchEval\", transformer=transformer, inputs=transform_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Create Processing Step to Evaluate Model Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# helper function to load json\n",
    "def load_json_by_line(file_path):\n",
    "    results_raw = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            results_raw.append(json.loads(line))\n",
    "    return results_raw\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load input data\n",
    "    transform_input_path = f\"/opt/ml/processing/transform_input/test.json\"\n",
    "    transform_input = load_json_by_line(transform_input_path)\n",
    "    \n",
    "    transform_path = f\"/opt/ml/processing/eval_transform/test.json.out\"\n",
    "    transform_results = load_json_by_line(transform_path)\n",
    "\n",
    "    # compile predictions with inputs\n",
    "    input_start_date = pd.to_datetime(transform_input[0]['start'])\n",
    "    predictions_available = False\n",
    "    predictions_data = []\n",
    "    for i, input in enumerate(transform_input):\n",
    "        start_date = pd.to_datetime(input['start'])\n",
    "        if(start_date > input_start_date):\n",
    "            predictions_available = True\n",
    "        store_nbr = int(input['cat'][0]) + 1\n",
    "        targets = input['target']\n",
    "\n",
    "        if(predictions_available):\n",
    "            predictions = transform_results[i]['mean']\n",
    "        else:\n",
    "            predictions = np.negative(np.ones(len(targets)))\n",
    "        \n",
    "        for j in range(7):\n",
    "            target = targets[j]\n",
    "            target_date = start_date + pd.Timedelta(days=j)\n",
    "            predictions_data.append([target_date, store_nbr, target, predictions[j]])\n",
    "\n",
    "    predictions_df = pd.DataFrame(columns=['date', 'store_nbr', 'true_sales', 'predicted_sales'], data=predictions_data)\n",
    "    predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
    "\n",
    "    # calculate RMSE\n",
    "    rmse = np.sqrt(np.mean(((predictions_df[54:]['true_sales'] - predictions_df[54:]['predicted_sales']) ** 2)))\n",
    "    std = np.std(predictions_df[54:]['true_sales'] - predictions_df[54:]['predicted_sales'])\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"rmse\": {\"value\": rmse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "model_eval = ScriptProcessor(\n",
    "    image_uri='343218227212.dkr.ecr.us-east-1.amazonaws.com/deepar-processing-container:latest',\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"deepar-pipeline-model-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "model_eval_args = model_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_transform_eval.properties.TransformOutput.S3OutputPath,\n",
    "            destination=\"/opt/ml/processing/eval_transform\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/transform_input\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processing step with property file output\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_model_eval = ProcessingStep(\n",
    "    name=\"DeepARPipelineModelEval\",\n",
    "    step_args=model_eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Register Model Step if Better than Current Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_model_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=registered_model_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "step_register_model = ModelStep(name=\"DeepARPipelineRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Batch Transform to Predict Next Weeks Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer\n",
    "from sagemaker.transformer import Transformer\n",
    "forecast_transformer = Transformer(\n",
    "    model_name=registered_model_name,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=\"{}/sales_forecast\".format(batch_data_path),\n",
    "    assemble_with=\"Line\",\n",
    "    env={\n",
    "        \"DEEPAR_INFERENCE_CONFIG\": json.dumps({\n",
    "            \"output_types\": [\"mean\"],\n",
    "        })\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TransformStep\n",
    "transform_input_forecast = TransformInput(\n",
    "    data=step_process.properties.ProcessingOutputConfig.Outputs[\"forecast\"].S3Output.S3Uri, \n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "# create a step if we run with existing model\n",
    "step_transform_forecast_existing = TransformStep(\n",
    "    name=\"DeepARPipelineBatchForecastExisting\", transformer=forecast_transformer, inputs=transform_input_forecast\n",
    ")\n",
    "\n",
    "# create a step if we run with new model\n",
    "step_transform_forecast_new = TransformStep(\n",
    "    name=\"DeepARPipelineBatchForecastNew\", transformer=forecast_transformer, inputs=transform_input_forecast\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conditional Step to Determine if Model Was Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_model_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.rmse.value\",\n",
    "    ),\n",
    "    right=rmse_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"DeepARPipelineCond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register_model, step_transform_forecast_new],\n",
    "    else_steps=[step_transform_forecast_existing],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"DeepARTrainDeployPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        rmse_threshold,\n",
    "        registered_model_name,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_eval_model, step_transform_eval, step_model_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'd80ecf38-c706-43fb-bbe0-ba4d92ee1747',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd80ecf38-c706-43fb-bbe0-ba4d92ee1747',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '93',\n",
       "   'date': 'Thu, 17 Oct 2024 19:35:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:343218227212:pipeline/DeepARTrainDeployPipeline/execution/ljo6r2cq50wl',\n",
       " 'PipelineExecutionDisplayName': 'execution-1729186621407',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'deepartraindeploypipeline',\n",
       "  'TrialName': 'ljo6r2cq50wl'},\n",
       " 'CreationTime': datetime.datetime(2024, 10, 17, 17, 37, 1, 353000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 10, 17, 17, 37, 1, 353000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:343218227212:user-profile/d-nzj1ohif3tlp/default-20241007T091581',\n",
       "  'UserProfileName': 'default-20241007T091581',\n",
       "  'DomainId': 'd-nzj1ohif3tlp',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::343218227212:assumed-role/AmazonSageMaker-ExecutionRole-20241007T091581/SageMaker',\n",
       "   'PrincipalId': 'AROAU72LGRAGGJLCV6WF3:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:343218227212:user-profile/d-nzj1ohif3tlp/default-20241007T091581',\n",
       "  'UserProfileName': 'default-20241007T091581',\n",
       "  'DomainId': 'd-nzj1ohif3tlp',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::343218227212:assumed-role/AmazonSageMaker-ExecutionRole-20241007T091581/SageMaker',\n",
       "   'PrincipalId': 'AROAU72LGRAGGJLCV6WF3:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': '1e185cb6-7fac-48b4-970a-4a6aded45f6d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1e185cb6-7fac-48b4-970a-4a6aded45f6d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1216',\n",
       "   'date': 'Thu, 17 Oct 2024 17:37:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'DeepARFeatureProcess',\n",
       "  'StartTime': datetime.datetime(2024, 10, 17, 17, 37, 2, 677000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Executing',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:343218227212:processing-job/pipelines-ljo6r2cq50wl-DeepARFeatureProcess-fCNkL6DS2M'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
